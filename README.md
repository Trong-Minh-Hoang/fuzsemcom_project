fuzsemcom_project/
â”œâ”€â”€ README.md                           # Project overview & quick start
â”œâ”€â”€ requirements.txt                    # Dependencies: pandas, scikit-fuzzy, numpy, matplotlib
â”‚
â”œâ”€â”€ src/                                # Core implementation (khÃ´ng cháº¡y trá»±c tiáº¿p)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fuzzy_engine.py                # Äá»•i tÃªn tá»« fuzzy_system_optimized.py
â”‚   â””â”€â”€ ground_truth_generator.py      # Logic táº¡o labels (tá»« 04_ground_truth.py)
â”‚
â”œâ”€â”€ scripts/                            # Scripts thá»±c thi theo pipeline
â”‚   â”œâ”€â”€ 01_data_exploration.py         # Explore dataset
â”‚   â”œâ”€â”€ 02_data_preprocessing.py       # Clean & filter data
â”‚   â”œâ”€â”€ 03_generate_ground_truth.py    # Generate semantic labels
â”‚   â”œâ”€â”€ 04_evaluate_fse.py             # Main evaluation (Ä‘á»•i tá»« 05_evaluate_fse.py)
â”‚   â”œâ”€â”€ 05_compare_with_deepsc.py      # Comparison (Ä‘á»•i tá»« 06_deepsc_comparison.py)
â”‚   â”œâ”€â”€ 06_ablation_study.py           # Optional: ablation analysis
â”‚   â””â”€â”€ debug_prediction.py            # Debug tool (tá»« debug_optimized.py)
â”‚
â”œâ”€â”€ data/                               # Data directory (gitignored náº¿u dataset lá»›n)
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ Agriculture_dataset_with_metadata.csv  # Dataset gá»‘c (download tá»« IEEE DataPort)
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ semantic_dataset.csv       # Output sau khi cháº¡y script 03
â”‚
â”œâ”€â”€ results/                            # Results & outputs (gitignored)
â”‚   â”œâ”€â”€ figures/                       # All plots & visualizations
â”‚   â”‚   â”œâ”€â”€ fse_confusion_matrix.png
â”‚   â”‚   â”œâ”€â”€ deepsc_confusion_matrix.png
â”‚   â”‚   â””â”€â”€ comparison_overview.png
â”‚   â””â”€â”€ reports/
â”‚       â”œâ”€â”€ fse_evaluation_results.json
â”‚       â”œâ”€â”€ deepsc_comparison_results.json
â”‚       â””â”€â”€ experiment_report.docx     # Auto-generated report
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ICC_ENGLAND_2026.pdf           # BÃ i bÃ¡o gá»‘c
â”‚   â””â”€â”€ student_guide_2026.pdf         # HÆ°á»›ng dáº«n (Ä‘á»•i tÃªn tá»« guide_2026.pdf)
â”‚
â””â”€â”€ .gitignore                         # Bá» qua data/raw, results/, *.pyc


pandas>=2.0.0
numpy>=1.24.0
scikit-fuzzy>=0.4.2
scikit-learn>=1.3.0
tensorflow>=2.13.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-optimize>=0.9.0

Step 1: Data Exploration (EDA)
   â†“
Step 2: Data Preprocessing  
   â†“
Step 3: Ground Truth Generation (Fuzzy Inference)
   â†“
Step 4: Evaluate FSE (Fuzzy Semantic Encoder)
   â†“
Step 5: Train Neural Decoder
   â†“
Step 6: Ablation Study (Bayesian Optimization)
   â†“
Step 7: Generate Final Report & Figures

STEP 1: DATA EXPLORATION
Má»¥c Ä‘Ã­ch:
Hiá»ƒu cáº¥u trÃºc dataset
Kiá»ƒm tra missing values
PhÃ¢n tÃ­ch distribution
Validate ranges
Cháº¡y:

cd scripts
python 01_data_exploration.py

âœ… CHECKPOINTS:
Console Output:
================================================================================
EXPLORATORY DATA ANALYSIS PIPELINE
================================================================================

LOADING DATASET
âœ“ Loaded dataset from: data/raw/Agriculture_dataset_with_metadata.csv
  Shape: 60,000 rows Ã— 24 columns
  Memory usage: 11.02 MB

DATASET OVERVIEW
Shape: 60,000 rows Ã— 24 columns
Column Names (24): Zone_ID, Image_Source_ID, ... [danh sÃ¡ch Ä‘áº§y Ä‘á»§]

MISSING VALUE ANALYSIS
âš ï¸  Found missing values in 5 columns:
  Migration_Timestamp: 57,234 (95.39%)
  NDRE: 30,000 (50.00%)
  ...

STATISTICAL SUMMARY
          Moisture          pH           N  Temperature    Humidity
count  60000.000000  60000.000  60000.000  60000.000000  60000.000
mean      22.456789    6.523     48.234      28.567890    62.345
...

RANGE VALIDATION
âœ“ Moisture: All values within [0, 100]
âœ“ pH: All values within [4.0, 9.0]
âœ“ N: All values within [0, 300]
âœ“ Temperature: All values within [10, 40]
âœ“ Humidity: All values within [30, 100]

LABEL DISTRIBUTION
NDI_Label Distribution:
  High  : 18,234 (30.4%)
  Medium: 24,567 (41.0%)
  Low   : 17,199 (28.7%)

âœ… EDA COMPLETE
Output directory: results/eda
Files Created:
Sao chÃ©p
results/eda/
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ missing_values.png          # âœ… Bar chart
â”‚   â”œâ”€â”€ sensor_distributions.png    # âœ… 5 histograms + boxplots
â”‚   â”œâ”€â”€ correlation_matrix.png      # âœ… Heatmap
â”‚   â””â”€â”€ label_distribution.png      # âœ… NDI/PDI/Semantic bars
â””â”€â”€ reports/
    â””â”€â”€ eda_report.txt              # âœ… Text summary
Kiá»ƒm tra:
Sao chÃ©p
# Check files exist
ls -lh results/eda/figures/
ls -lh results/eda/reports/

# View report
cat results/eda/reports/eda_report.txt
Expected Results (so vá»›i paper):
Metric	Expected	Your Result	âœ…/âŒ
Total samples	~60,000	?	
Moisture range	[0, 100]	?	
pH range	[4.0, 9.0]	?	
N range	[0, 300]	?	
Temp range	[10, 40]	?	
Humidity range	[30, 100]	?	

STEP 2: DATA PREPROCESSING
Má»¥c Ä‘Ã­ch:
Map NDI/PDI labels â†’ semantic symbols
Apply priority hierarchy (Table III)
Validate fuzzy inference
Cháº¡y:
Sao chÃ©p
python 02_data_preprocessing.py
âœ… CHECKPOINTS:
Console Output:
Sao chÃ©p
================================================================================
DATA PREPROCESSING PIPELINE
================================================================================

[1/5] Loading raw data...
âœ“ Loaded 60,000 samples

[2/5] Validating data...
âœ“ Removed 1,579 rows with missing values
âœ“ Removed 312 rows with out-of-range values
âœ“ Final dataset: 58,109 valid samples

[3/5] Mapping NDI/PDI labels to semantic symbols...
âœ“ Applied priority hierarchy (Table III)
âœ“ Label distribution:
    optimal                    : 14,080 (24.2%)
    water_deficit_acidic       : 10,457 (18.0%)
    water_deficit_alkaline     :  8,645 (14.9%)
    nutrient_deficiency        :  7,186 (12.4%)
    fungal_risk                :  5,375 ( 9.2%)
    acidic_soil                :  5,083 ( 8.7%)
    alkaline_soil              :  4,148 ( 7.1%)
    heat_stress                :  3,447 ( 5.9%)
    other                      :  3,688 ( 6.3%)

[4/5] Validating with fuzzy inference...
âœ“ Fuzzy agreement: 88.7%

[5/5] Saving preprocessed data...
âœ“ Saved to data/processed/semantic_dataset_preprocessed.csv

âœ… PREPROCESSING COMPLETE
Files Created:
Sao chÃ©p
data/processed/
â”œâ”€â”€ semantic_dataset_preprocessed.csv   # âœ… Main output
â””â”€â”€ preprocessing_stats.txt             # âœ… Statistics
Kiá»ƒm tra:
Sao chÃ©p
# Check file
head -20 data/processed/semantic_dataset_preprocessed.csv

# Check stats
cat data/processed/preprocessing_stats.txt

# Verify label distribution
python -c "
import pandas as pd
df = pd.read_csv('data/processed/semantic_dataset_preprocessed.csv')
print(df['semantic_label'].value_counts())
"
Expected Results (so vá»›i paper Table IV):
Label	Expected %	Your %	âœ…/âŒ
optimal	24.1%	?	
water_deficit_acidic	17.9%	?	
water_deficit_alkaline	14.8%	?	
nutrient_deficiency	12.3%	?	
fungal_risk	9.2%	?	
acidic_soil	8.7%	?	
alkaline_soil	7.1%	?	
heat_stress	5.9%	?	
ðŸ”¥ STEP 3: GROUND TRUTH GENERATION
Má»¥c Ä‘Ã­ch:
Generate semantic labels using fuzzy inference
Split train/test (80/20)
Calculate confidence scores
Cháº¡y:
Sao chÃ©p
python ground_truth_generator.py
âœ… CHECKPOINTS:
Console Output:
Sao chÃ©p
================================================================================
FUZSEMCOM GROUND TRUTH GENERATION PIPELINE
================================================================================

[1/6] Loading raw data...
âœ“ Loaded 60,000 samples

[2/6] Validating data...
âœ“ Final dataset: 58,421 valid samples

[3/6] Generating semantic labels...
ðŸ”® Generating semantic labels using fuzzy inference...
âœ“ Fuzzy system initialized (expert-defined membership functions)
âœ“ Generated labels for 58,421 samples

[4/6] Saving full labeled dataset...
âœ“ Saved to data/processed/semantic_dataset_fuzzy.csv

[5/6] Splitting train/test (80/20)...
âœ“ Train: 46,736 samples â†’ semantic_dataset_train.csv
âœ“ Test:  11,685 samples â†’ semantic_dataset_test.csv

[6/6] Generating statistics...
LABEL DISTRIBUTION
optimal                        14,080 (24.1%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
water_deficit_acidic           10,457 (17.9%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
...

CONFIDENCE STATISTICS
Mean Confidence:    187.3/255 (73.5%)
Median Confidence:  201.0/255
Min Confidence:     45/255
Max Confidence:     255/255

âœ… GROUND TRUTH GENERATION COMPLETE
Files Created:
Sao chÃ©p
data/processed/
â”œâ”€â”€ semantic_dataset_fuzzy.csv          # âœ… Full dataset
â”œâ”€â”€ semantic_dataset_train.csv          # âœ… Training split
â”œâ”€â”€ semantic_dataset_test.csv           # âœ… Test split
â””â”€â”€ fuzzy_generation_stats.txt          # âœ… Statistics
Kiá»ƒm tra:
Sao chÃ©p
# Check file sizes
wc -l data/processed/semantic_dataset_*.csv

# Expected:
# 58,422 semantic_dataset_fuzzy.csv (header + 58,421 rows)
# 46,737 semantic_dataset_train.csv
# 11,686 semantic_dataset_test.csv

# Check columns
head -1 data/processed/semantic_dataset_fuzzy.csv

# Expected columns:
# soil_moisture,pH,nitrogen,temperature,humidity,semantic_label,confidence

# Verify confidence distribution
python -c "
import pandas as pd
df = pd.read_csv('data/processed/semantic_dataset_fuzzy.csv')
print('Mean confidence:', df['confidence'].mean())
print('Median confidence:', df['confidence'].median())
"
Expected Results:
Metric	Expected	Your Result	âœ…/âŒ
Total samples	~58,000	?	
Train samples	~46,000	?	
Test samples	~11,000	?	
Mean confidence	180-190/255	?	
Label: optimal	~24%	?	
ðŸ”¥ STEP 4: EVALUATE FSE (Fuzzy Semantic Encoder)
Má»¥c Ä‘Ã­ch:
Evaluate fuzzy inference accuracy
Generate confusion matrix
Analyze confidence scores
Compare with paper (88.7%)
Cháº¡y:
Sao chÃ©p
python 04_evaluate_fse.py
âœ… CHECKPOINTS:
Console Output:
Sao chÃ©p
================================================================================
FUZZY SEMANTIC ENCODER EVALUATION
================================================================================

LOADING TEST DATA
âœ“ Loaded test dataset: data/processed/semantic_dataset_test.csv
  Samples: 11,685

RUNNING FUZZY INFERENCE
âœ“ Fuzzy system initialized
Processing 11,685 samples...
  Progress: 1,000/11,685 (8.6%)
  ...
âœ“ Inference complete

CALCULATING METRICS
âœ“ Overall Accuracy: 88.73%
  Expected (paper): 88.70%
  Difference:       +0.03%

âœ“ Confidence Statistics:
  Mean:   187.3/255 (73.5%)
  Median: 201.0/255
  Std:    42.1
  Range:  [45, 255]

GENERATING VISUALIZATIONS
âœ“ Saved confusion matrix to results/figures/fse_confusion_matrix.png
âœ“ Saved per-class metrics to results/figures/fse_per_class_metrics.png
âœ“ Saved confidence distribution to results/figures/fse_confidence_distribution.png
âœ“ Saved symbol distribution to results/figures/fse_symbol_distribution.png

SAVING RESULTS
âœ“ Saved JSON results to results/reports/fse_evaluation_results.json
âœ“ Saved text report to results/reports/fse_evaluation_report.txt

================================================================================
EVALUATION COMPLETE
================================================================================
Overall Accuracy:     88.73%
Expected (paper):     88.70%
Difference:           +0.03%
Mean Confidence:      187.3/255
Inference Errors:     0
Files Created:
Sao chÃ©p
results/
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ fse_confusion_matrix.png          # âœ… Normalized heatmap
â”‚   â”œâ”€â”€ fse_per_class_metrics.png         # âœ… Precision/Recall/F1 bars
â”‚   â”œâ”€â”€ fse_confidence_distribution.png   # âœ… Histogram + boxplot
â”‚   â””â”€â”€ fse_symbol_distribution.png       # âœ… Symbol frequency
â””â”€â”€ reports/
    â”œâ”€â”€ fse_evaluation_results.json       # âœ… JSON metrics
    â””â”€â”€ fse_evaluation_report.txt         # âœ… Text report
Kiá»ƒm tra:
Sao chÃ©p
# View JSON results
cat results/reports/fse_evaluation_results.json | python -m json.tool

# View text report
cat results/reports/fse_evaluation_report.txt

# Check accuracy
python -c "
import json
with open('results/reports/fse_evaluation_results.json') as f:
    data = json.load(f)
    print(f\"Accuracy: {data['accuracy']*100:.2f}%\")
    print(f\"Expected: {data['paper_comparison']['expected_accuracy']*100:.2f}%\")
"
Expected Results (so vá»›i paper Section IV.D):
Metric	Expected	Your Result	âœ…/âŒ
Overall Accuracy	88.7%	?	
Optimal Precision	~92%	?	
Optimal Recall	~91%	?	
Mean Confidence	180-190/255	?	
Inference Errors	0	?	
Visual Checks:
Sao chÃ©p
# Open figures
open results/figures/fse_confusion_matrix.png
open results/figures/fse_per_class_metrics.png
Confusion matrix should show:

Diagonal values > 0.85 (high accuracy)
Off-diagonal values < 0.10 (low confusion)
Optimal class: highest accuracy (~92%)
ðŸ”¥ STEP 5: TRAIN NEURAL DECODER
Má»¥c Ä‘Ã­ch:
Train LSTM decoder (symbol â†’ sensor values)
Evaluate reconstruction accuracy
Compare with paper (RMSE, MAE)
Cháº¡y:
Sao chÃ©p
python 05_train_neural_decoder.py
âœ… CHECKPOINTS:
Console Output:
Sao chÃ©p
================================================================================
NEURAL DECODER TRAINING PIPELINE
================================================================================

[1/6] Loading data...
âœ“ Train: 46,736 samples
âœ“ Test:  11,685 samples

[2/6] Encoding symbols...
âœ“ Encoded 9 unique symbols

[3/6] Building model...
Model: "sequential"
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
embedding (Embedding)       (None, 1, 64)             576       
lstm (LSTM)                 (None, 128)               98816     
dense (Dense)               (None, 64)                8256      
dense_1 (Dense)             (None, 5)                 325       
=================================================================
Total params: 108,973
Trainable params: 108,973

[4/6] Training model...
Epoch 1/50
1460/1460 [==============================] - 12s - loss: 0.0234 - val_loss: 0.0156
Epoch 2/50
1460/1460 [==============================] - 11s - loss: 0.0145 - val_loss: 0.0132
...
Epoch 50/50
1460/1460 [==============================] - 11s - loss: 0.0089 - val_loss: 0.0091

âœ“ Training complete (best epoch: 47)

[5/6] Evaluating model...
Test Loss: 0.0091

Per-Variable RMSE:
  soil_moisture: 3.45
  pH:            0.23
  nitrogen:      8.67
  temperature:   1.89
  humidity:      4.12

Per-Variable MAE:
  soil_moisture: 2.78
  pH:            0.18
  nitrogen:      6.89
  temperature:   1.45
  humidity:      3.34

[6/6] Saving results...
âœ“ Model saved to models/neural_decoder.h5
âœ“ Results saved to results/reports/neural_decoder_results.json

================================================================================
TRAINING COMPLETE
================================================================================
Test RMSE: 4.23
Test MAE:  3.12
Files Created:
Sao chÃ©p
models/
â””â”€â”€ neural_decoder.h5                    # âœ… Trained model

results/
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ training_history.png            # âœ… Loss curves
â”‚   â”œâ”€â”€ reconstruction_error.png        # âœ… Error distribution
â”‚   â””â”€â”€ prediction_vs_actual.png        # âœ… Scatter plots
â””â”€â”€ reports/
    â””â”€â”€ neural_decoder_results.json     # âœ… Metrics
Kiá»ƒm tra:
Sao chÃ©p
# Check model file
ls -lh models/neural_decoder.h5

# View results
cat results/reports/neural_decoder_results.json | python -m json.tool

# Check RMSE
python -c "
import json
with open('results/reports/neural_decoder_results.json') as f:
    data = json.load(f)
    print(f\"Overall RMSE: {data['test_rmse']:.2f}\")
    for var, rmse in data['per_variable_rmse'].items():
        print(f\"  {var}: {rmse:.2f}\")
"
Expected Results (so vá»›i paper Section IV.E):
Metric	Expected	Your Result	âœ…/âŒ
Overall RMSE	4.2 Â± 0.3	?	
Overall MAE	3.1 Â± 0.2	?	
Moisture RMSE	3.4 Â± 0.2	?	
pH RMSE	0.23 Â± 0.05	?	
Nitrogen RMSE	8.6 Â± 0.5	?	
Temperature RMSE	1.9 Â± 0.2	?	
Humidity RMSE	4.1 Â± 0.3	?	
Visual Checks:
Sao chÃ©p
open results/figures/training_history.png
Training curves should show:

Loss decreasing smoothly
No overfitting (train/val loss similar)
Convergence around epoch 40-50
ðŸ”¥ STEP 6: ABLATION STUDY (Bayesian Optimization)
Má»¥c Ä‘Ã­ch:
Optimize membership function parameters
Compare baseline vs optimized
Validate improvement
âš ï¸ WARNING: This step takes 2-4 hours to run!
Cháº¡y:
Sao chÃ©p
python 06_ablation_study.py
âœ… CHECKPOINTS:
Console Output:
Sao chÃ©p
================================================================================
ABLATION STUDY: BAYESIAN OPTIMIZATION
================================================================================

[1/5] Loading validation data...
âœ“ Loaded 5,842 validation samples (10% of train)

[2/5] Evaluating baseline...
âœ“ Baseline accuracy: 88.73%

[3/5] Running Bayesian Optimization...
Iteration 1/50: Current best = -0.8873
Iteration 2/50: Current best = -0.8891
Iteration 3/50: Current best = -0.8912
...
Iteration 50/50: Current best = -0.9045

âœ“ Optimization complete

[4/5] Evaluating optimized system...
âœ“ Optimized accuracy: 90.45%

[5/5] Saving results...
âœ“ Best params saved to results/reports/bo_best_params.json
âœ“ Optimization history saved to results/reports/bo_history.csv

================================================================================
ABLATION STUDY COMPLETE
================================================================================
Baseline Accuracy:   88.73%
Optimized Accuracy:  90.45%
Improvement:         +1.72%
Files Created:
Sao chÃ©p
results/
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ bo_convergence.png              # âœ… Optimization curve
â”‚   â””â”€â”€ bo_parameter_importance.png     # âœ… Feature importance
â””â”€â”€ reports/
    â”œâ”€â”€ bo_best_params.json             # âœ… Optimized params
    â”œâ”€â”€ bo_history.csv                  # âœ… All iterations
    â””â”€â”€ ablation_study_report.txt       # âœ… Summary
Kiá»ƒm tra:
Sao chÃ©p
# View best params
cat results/reports/bo_best_params.json | python -m json.tool

# View improvement
python -c "
import json
with open('results/reports/bo_best_params.json') as f:
    data = json.load(f)
    print(f\"Baseline:  {data['baseline_accuracy']*100:.2f}%\")
    print(f\"Optimized: {data['optimized_accuracy']*100:.2f}%\")
    print(f\"Improvement: {data['improvement']*100:+.2f}%\")
"

# Check convergence
tail -20 results/reports/bo_history.csv
Expected Results (so vá»›i paper Section IV.F):
Metric	Expected	Your Result	âœ…/âŒ
Baseline Accuracy	88.7%	?	
Optimized Accuracy	90.0-91.0%	?	
Improvement	+1.5-2.5%	?	
Convergence	< 50 iterations	?	
ðŸ”¥ STEP 7: GENERATE FINAL REPORT
Má»¥c Ä‘Ã­ch:
Tá»•ng há»£p táº¥t cáº£ káº¿t quáº£
So sÃ¡nh vá»›i paper
Generate publication-ready figures
Cháº¡y:
Sao chÃ©p
python 07_generate_report.py  # (Náº¿u cÃ³ script nÃ y)
# HOáº¶C tá»± tá»•ng há»£p:
Manual Report Generation:
Sao chÃ©p
# create_final_report.py
import json
import pandas as pd

print("="*80)
print("FUZSEMCOM FINAL RESULTS SUMMARY")
print("="*80)

# Load all results
with open('results/reports/fse_evaluation_results.json') as f:
    fse_results = json.load(f)

with open('results/reports/neural_decoder_results.json') as f:
    decoder_results = json.load(f)

with open('results/reports/bo_best_params.json') as f:
    bo_results = json.load(f)

# Print summary
print("\n1. FUZZY SEMANTIC ENCODER (FSE)")
print("-"*80)
print(f"Accuracy:        {fse_results['accuracy']*100:.2f}%")
print(f"Expected (paper): 88.70%")
print(f"Difference:      {(fse_results['accuracy']-0.887)*100:+.2f}%")

print("\n2. NEURAL DECODER")
print("-"*80)
print(f"Overall RMSE:    {decoder_results['test_rmse']:.2f}")
print(f"Overall MAE:     {decoder_results['test_mae']:.2f}")
print(f"Expected RMSE:   4.2 Â± 0.3")

print("\n3. ABLATION STUDY")
print("-"*80)
print(f"Baseline:        {bo_results['baseline_accuracy']*100:.2f}%")
print(f"Optimized:       {bo_results['optimized_accuracy']*100:.2f}%")
print(f"Improvement:     {bo_results['improvement']*100:+.2f}%")

print("\n" + "="*80)
print("âœ… ALL EXPERIMENTS COMPLETE")
print("="*80)
Sao chÃ©p
python create_final_report.py
ðŸ“Š FINAL CHECKPOINT TABLE
So sÃ¡nh vá»›i Paper:
Metric	Paper	Your Result	Status
Section IV.D: FSE Accuracy			
Overall Accuracy	88.7%	?	âœ…/âŒ
Optimal Precision	92%	?	âœ…/âŒ
Mean Confidence	185/255	?	âœ…/âŒ
Section IV.E: Neural Decoder			
Overall RMSE	4.2	?	âœ…/âŒ
Overall MAE	3.1	?	âœ…/âŒ
Moisture RMSE	3.4	?	âœ…/âŒ
pH RMSE	0.23	?	âœ…/âŒ
Section IV.F: Ablation Study			
Baseline Accuracy	88.7%	?	âœ…/âŒ
Optimized Accuracy	90.0-91.0%	?	âœ…/âŒ
Improvement	+1.5-2.5%	?	âœ…/âŒ
ðŸŽ¯ QUICK VERIFICATION SCRIPT
Táº¡o file verify_all_results.py:

Sao chÃ©p
"""
verify_all_results.py - Quick verification of all experiments
"""

import json
from pathlib import Path

def check_file(path, description):
    if Path(path).exists():
        print(f"âœ… {description}: {path}")
        return True
    else:
        print(f"âŒ {description}: {path} NOT FOUND")
        return False

def check_metric(value, expected_min, expected_max, name):
    if expected_min <= value <= expected_max:
        print(f"âœ… {name}: {value:.2f} (within [{expected_min}, {expected_max}])")
        return True
    else:
        print(f"âŒ {name}: {value:.2f} (outside [{expected_min}, {expected_max}])")
        return False

print("="*80)
print("FUZSEMCOM RESULTS VERIFICATION")
print("="*80)

all_pass = True

# Check files
print("\n1. CHECKING FILES...")
all_pass &= check_file('data/processed/semantic_dataset_train.csv', 'Train data')
all_pass &= check_file('data/processed/semantic_dataset_test.csv', 'Test data')
all_pass &= check_file('results/reports/fse_evaluation_results.json', 'FSE results')
all_pass &= check_file('results/reports/neural_decoder_results.json', 'Decoder results')
all_pass &= check_file('models/neural_decoder.h5', 'Trained model')

# Check FSE metrics
print("\n2. CHECKING FSE METRICS...")
try:
    with open('results/reports/fse_evaluation_results.json') as f:
        fse = json.load(f)
    all_pass &= check_metric(fse['accuracy']*100, 87.0, 90.0, 'FSE Accuracy')
    all_pass &= check_metric(fse['confidence_statistics']['mean'], 170, 200, 'Mean Confidence')
except Exception as e:
    print(f"âŒ Error loading FSE results: {e}")
    all_pass = False

# Check Decoder metrics
print("\n3. CHECKING DECODER METRICS...")
try:
    with open('results/reports/neural_decoder_results.json') as f:
        decoder = json.load(f)
    all_pass &= check_metric(decoder['test_rmse'], 3.5, 5.0, 'Overall RMSE')
    all_pass &= check_metric(decoder['test_mae'], 2.5, 4.0, 'Overall MAE')
except Exception as e:
    print(f"âŒ Error loading decoder results: {e}")
    all_pass = False

# Final verdict
print("\n" + "="*80)
if all_pass:
    print("âœ… ALL CHECKS PASSED - RESULTS MATCH PAPER")
else:
    print("âŒ SOME CHECKS FAILED - REVIEW ABOVE")
print("="*80)
Sao chÃ©p
python verify_all_results.py
ðŸš¨ TROUBLESHOOTING
Common Issues:
1. Import Error:
Sao chÃ©p
ModuleNotFoundError: No module named 'skfuzzy'
Fix:

Sao chÃ©p
pip install scikit-fuzzy
2. File Not Found:
Sao chÃ©p
FileNotFoundError: data/raw/Agriculture_dataset_with_metadata.csv
Fix:

Sao chÃ©p
# Äáº£m báº£o file CSV á»Ÿ Ä‘Ãºng vá»‹ trÃ­
ls data/raw/
3. Low Accuracy (<85%):
Possible causes:

Wrong column mapping
Missing data preprocessing
Incorrect fuzzy rules
Debug:

Sao chÃ©p
# Check label distribution
import pandas as pd
df = pd.read_csv('data/processed/semantic_dataset_train.csv')
print(df['semantic_label'].value_counts(normalize=True))
4. High RMSE (>6.0):
Possible causes:

Insufficient training epochs
Wrong normalization
Model architecture issues
Debug:

Sao chÃ©p
# Check training history
import json
with open('results/reports/neural_decoder_results.json') as f:
    data = json.load(f)
    print("Training epochs:", data.get('epochs_trained'))
    print("Best epoch:", data.get('best_epoch'))
âœ… SUCCESS CRITERIA
Báº¡n Ä‘Ã£ hoÃ n thÃ nh thÃ nh cÃ´ng khi:

âœ… All 7 steps run without errors
âœ… FSE accuracy: 87-90%
âœ… Decoder RMSE: 3.5-5.0
âœ… BO improvement: +1-3%
âœ… All figures generated
âœ… All reports created
âœ… Results match paper (Â±2%)

